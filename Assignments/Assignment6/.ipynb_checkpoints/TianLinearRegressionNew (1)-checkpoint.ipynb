{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression from Scratch in Python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2. In the *loss* function, the prediction error (residual) is evaluated for each observation. \n",
    "1. What are the elements in the returned `res` array?\n",
    "2. In the *loss* function defined below, what would `np.dot(X, w)+b` return?\n",
    "\n",
    "**Your Answers:** *fill in here* \n",
    "\n",
    "* `res` array will return the difference between observation and prediction of y.\n",
    "\n",
    "* `np.dot(X, w)+b` will return `y_pred += w[j] * X[i][j]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function computes and returns SSE(the squared sum of the residuals) divided by the number of observations and the residuals\n",
    "def loss(y, X, w, b):\n",
    "    \"\"\" Unvectorized version. \"\"\"\n",
    "    loss = 0\n",
    "    res = np.zeros(X.shape[0])\n",
    "    for i in range(X.shape[0]):\n",
    "        # evaluate the predicted value of y for each observation\n",
    "        y_pred = b\n",
    "        for j in range(X.shape[1]):\n",
    "            y_pred += w[j] * X[i][j]\n",
    "        res[i] = y[i] - y_pred\n",
    "        loss += res[i] ** 2.0\n",
    "    return loss/X.shape[0], res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3. In the *grd* function, the gradient of the loss function in terms of `w` and `b` is returned.\n",
    "1. What is the shape of `dw`? \n",
    "2. Is `db` an array or a scalar?\n",
    "3. In the grd function defined below, what would `-2 * np.dot(X.T, res) / X.shape[0]` return?\n",
    "4. In the grd function defined below, what would `-2 * np.sum(res) / X.shape[0]` return?\n",
    "\n",
    "**Your Answers:** *fill in here*\n",
    "\n",
    "* `dw` has the same shape like w.\n",
    "* `db` is an a scalar.\n",
    "* `-2 * np.dot(X.T, res) / X.shape[0]` will return `dw/X.shape[0]`\n",
    "* `-2 * np.sum(res) / X.shape[0]` will return `db/X.shape[0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grd function computes and returns the gradient (dw, db)\n",
    "def grd(X, w, b, res):\n",
    "    \"\"\" Unvectorized version. \"\"\"\n",
    "    # dw contains the partial derivatives of the loss to w\n",
    "    dw = np.zeros_like(w)\n",
    "    # db contains the derivative of the loss to b\n",
    "    db = 0\n",
    "    for j in range(X.shape[1]):\n",
    "        for i in range(X.shape[0]):\n",
    "            dw[j] -= 2 * res[i] * X[i][j] \n",
    "    for i in range(X.shape[0]):\n",
    "        db -= 2 * res[i]\n",
    "    return dw/X.shape[0], db/X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4. In the *grd_descent* function, gradient descent is applied to update parameters `w` and `b`.\n",
    "1. What does argument `iter` represent? \n",
    "2. In the grd_descent function defined below, what would `w -= lr * dw` return?\n",
    "\n",
    "**Your Answers:** *fill in here*\n",
    "* `iter` represent how many time the function will repeats.\n",
    "* `w -= lr * dw` wii return  `w[j] -= lr * dw[j]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grd_descent function applies gradient descent\n",
    "def grd_descent(y, X, w, b, lr = 1.0e-5, iter = 100):\n",
    "    \"\"\" Unvectorized version. \"\"\"\n",
    "    for t in range(iter):\n",
    "        cost, res = loss(y, X, w, b)\n",
    "        print(\"Iteration %d: Loss = %.4f\" %(t+1, cost))\n",
    "        dw, db = grd(X, w, b, res)\n",
    "        for j in range(X.shape[1]):\n",
    "            w[j] -= lr * dw[j]\n",
    "        b -= lr * db\n",
    "    return w, b, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_vectorized function computes and returns SSE(the squared sum of the residuals) divided by the number of observations and the residuals\n",
    "def loss_vectorized(y, X, w, b):\n",
    "    \"\"\" Vectorized version. \"\"\"\n",
    "    loss = 0\n",
    "    res = np.zeros(X.shape[0])\n",
    "    y_pred = np.dot(X, w) + b\n",
    "    ####################################\n",
    "    # TODO 1:                          #\n",
    "    # Compute the residuals and store  #\n",
    "    # the results in res. Then compute #\n",
    "    # SSE and store the result in loss.#\n",
    "    # Use only basic Numpy array       #\n",
    "    # operations. No loop is allowed.  #\n",
    "    ####################################\n",
    "    res = y - y_pred\n",
    "    loss = np.sum(res ** 2.0)\n",
    "    pass\n",
    "    return loss/X.shape[0], res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grd_vectorized function computes and returns the gradient (dw, db)\n",
    "def grd_vectorized(X, w, b, res):\n",
    "    \"\"\" Vectorized version. \"\"\"\n",
    "    # dw contains the partial derivatives of the loss to w\n",
    "    dw = np.zeros_like(w)\n",
    "    # db contains the derivative of the loss to b\n",
    "    db = 0\n",
    "    dw -= 2 * np.dot(X.T, res)\n",
    "    ####################################\n",
    "    # TODO 2:                          #\n",
    "    # Compute the gradient of the loss #\n",
    "    # function in terms of b multiplied#\n",
    "    # by the number of observations    #\n",
    "    # and store the result in db. Use  #\n",
    "    # only basic Numpy array           #\n",
    "    # operations. No loop is allowed.  #\n",
    "    ####################################\n",
    "    db -=2 * np.sum(res)\n",
    "    pass\n",
    "    return dw/X.shape[0], db/X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grd_descent_vectorized function applies gradient descent\n",
    "def grd_descent_vectorized(y, X, w, b, lr = 1.0e-5, iter = 100):\n",
    "    \"\"\" Vectorized version. \"\"\"\n",
    "    for t in range(iter):\n",
    "        cost, res = loss_vectorized(y, X, w, b)\n",
    "        print(\"Iteration %d: Loss = %.4f\" %(t+1, cost))\n",
    "        dw, db = grd_vectorized(X, w, b, res)\n",
    "        w -= lr * dw\n",
    "        b -= lr * db\n",
    "    return w, b, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size:\n",
      "Number of observations:   5\n",
      "Number of features:  2\n"
     ]
    }
   ],
   "source": [
    "# dataset, 2 features and 5 observations\n",
    "x1 = np.array([48, 62, 79, 76, 59])\n",
    "x2 = np.array([68, 81, 80, 83, 64])\n",
    "# y is a vector containing the values of the dependent variable\n",
    "y = np.array([63, 72, 78, 79, 62])\n",
    "# X is a 5 by 2 matrix containing the values of the features(independent variables) \n",
    "X = np.array([x1, x2]).T\n",
    "print(\"Dataset Size:\")\n",
    "print(\"Number of observations: %3d\" %X.shape[0])\n",
    "print(\"Number of features: %2d\" %X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1. In the next code cell, Numpy arrays `X` and `y` constructed in the last code cell are used to train the linear regression model. Click the *Cell* tab, and select *Run All* to train the model. You shall see that loss (SSE/n, the squared sum of the residuals divided by the number of observations) is decreasing, which means that the parameters `w` and `b` are updated iteratively to reduce prediction errors.\n",
    "1. What does `X.shape[0]` return?\n",
    "2. What does `X.shape[1]` return?\n",
    "3. Examine the last statement in the next code cell. Interpret root mean square error (RMSE).\n",
    "\n",
    "**Your Answers:** *fill in here*\n",
    "* 5\n",
    "* 2\n",
    "* RMSE is a measure of how spread out these residuals are. In other words, it tells you how concentrated the data is around the line of best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Loss = 5044.3427\n",
      "Iteration 2: Loss = 4.2451\n",
      "Iteration 3: Loss = 4.2233\n",
      "Iteration 4: Loss = 4.2057\n",
      "Iteration 5: Loss = 4.1881\n",
      "Returned parameters:\n",
      "Coefficients:  [0.46306511 0.53954201]\n",
      "Bias:  0.007145230408703596\n",
      "Regression Equation:\n",
      "y = 0.0071 + 0.4631X1 + 0.5395X2 \n",
      "Root Mean Square Error(RMSE): 2.0465\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "# initialization; w is a vector containing the coefficients; b is a scalar equal to the bias\n",
    "w = np.ones(X.shape[1]) * 1.0e-3\n",
    "b = 0\n",
    "# learning rate\n",
    "lr = 5.0e-5\n",
    "# call the grd_descent function to learn the parameters\n",
    "w, b, loss = grd_descent(y, X, w, b, lr, iter = 5)\n",
    "print('Returned parameters:')\n",
    "print('Coefficients: ', w)\n",
    "print('Bias: ', b)\n",
    "print('Regression Equation:')\n",
    "print('y = %.4f ' %b, end = '')\n",
    "for i in range(X.shape[1]):\n",
    "    print('+ %.4fX%-2d' %(w[i], i+1), end = '')\n",
    "print('\\nRoot Mean Square Error(RMSE): %.4f' %(np.sqrt(loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5. Complete *TODO 1* and *TODO 2* in functions *loss_vectorized* and *grd_vectorized*. Then run the code below (hint: click the *Cell* tab, and select *Run All*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Loss = 5044.3427\n",
      "Iteration 2: Loss = 4.2451\n",
      "Iteration 3: Loss = 4.2233\n",
      "Iteration 4: Loss = 4.2057\n",
      "Iteration 5: Loss = 4.1881\n",
      "Returned parameters:\n",
      "Coefficients:  [0.46306511 0.53954201]\n",
      "Bias:  0.007145230408703595\n",
      "Regression Equation:\n",
      "y = 0.0071 + 0.4631X1 + 0.5395X2 \n",
      "Root Mean Square Error(RMSE): 2.0465\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "# initialization; w is a vector containing the coefficients; b is a scalar equal to the bias\n",
    "w = np.ones(X.shape[1]) * 1.0e-3\n",
    "b = 0\n",
    "# learning rate\n",
    "lr = 5.0e-5\n",
    "# call the grd_descent function to learn the parameters\n",
    "w, b, loss = grd_descent_vectorized(y, X, w, b, lr, iter = 5)\n",
    "print('Returned parameters:')\n",
    "print('Coefficients: ', w)\n",
    "print('Bias: ', b)\n",
    "print('Regression Equation:')\n",
    "print('y = %.4f ' %b, end = '')\n",
    "for i in range(X.shape[1]):\n",
    "    print('+ %.4fX%-2d' %(w[i], i+1), end = '')\n",
    "print('\\nRoot Mean Square Error(RMSE): %.4f' %(np.sqrt(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
