{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Linear Regression and Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smsize = 10\n",
    "np.random.seed(111)\n",
    "x = np.random.rand(smsize)\n",
    "y = 0.3 + 0.5 * x + np.random.rand(smsize)\n",
    "print('x\\ty')\n",
    "for i in range(smsize):\n",
    "    print('%.4f\\t%.4f' %(x[i], y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a scatter plot\n",
    "fig = plt.figure(figsize = (10, 5))   \n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(x, y, 'o')\n",
    "# simple linear regression\n",
    "LR = np.polyfit(x, y, 1)\n",
    "# a least-squares regression line\n",
    "x2 = np.linspace(0, 1, 11)\n",
    "y2 = np.poly1d(LR)(x2)\n",
    "ax.plot(x2, y2, '-', lw=3)\n",
    "# visualizing the residuals\n",
    "y_pred = np.poly1d(LR)(x)\n",
    "res = y - y_pred\n",
    "yerr_lm = res.copy()\n",
    "yerr_lm[res > 0] = 0\n",
    "yerr_lm[res < 0] = -1 * yerr_lm[res < 0]\n",
    "yerr_um = res.copy()\n",
    "yerr_um[res <= 0] = 0\n",
    "ax.errorbar(x, np.poly1d(LR)(x), yerr=[yerr_lm, yerr_um], marker='s', fmt='o', elinewidth=2)\n",
    "# configuring the plot\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_title('Simple Linear Regression')\n",
    "ax.set_ylim(0, 2)\n",
    "ax.text(0.82, 1.35, 'y='+'%.2f'%LR[1]+'+'+'%.2f'%LR[0]+'x')\n",
    "ax.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Tradeoff\n",
    "- Error due to bias is the difference between the average prediction of our model and the actual (correct) value we are trying to predict. A model with high bias pays little attention to training data and is oversimplified. It leads to high errors on both training and test data. \n",
    "- Error due to variance is the variation in model prediction for a given data point. A model with high variance pays a lot of attention to training data and does not generalize on the data it hasn't seen before. It performs well on training data but has high errors on test data.\n",
    "- An unfitting model is too simple and has few parameters. It may have high bias and low variance.\n",
    "- An overfitting model is too complex and has too many parameters. It may have low bias and high variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# polynomial regression equations of different degrees\n",
    "fig = plt.figure(figsize = (10, 5))   \n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(x, y, 'o')\n",
    "x2 = np.linspace(0, 1, 101)\n",
    "# baseline; degree = 0\n",
    "ax.axhline(np.mean(y), xmin=0, xmax=1, linestyle='-')\n",
    "# degree = 1\n",
    "poly_1 = np.poly1d(np.polyfit(x, y, 1))\n",
    "ax.plot(x2, poly_1(x2), '-.')\n",
    "# degree = smsize-1\n",
    "poly_s = np.poly1d(np.polyfit(x, y, smsize-1))\n",
    "ax.plot(x2, poly_s(x2), '--')\n",
    "# configuring the figure\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_title('')\n",
    "ax.set_ylim(-3, 8)\n",
    "ax.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression\n",
    "para_history = []\n",
    "loss_history = []\n",
    "# learning rate\n",
    "lr = 0.2\n",
    "b0 = -5\n",
    "b1 = -5\n",
    "for t in range(400):\n",
    "    y_pred = b0 + b1 * x\n",
    "    res = y - y_pred\n",
    "    loss = np.sum(np.square(res)) / y.shape[0]\n",
    "    para_history.append((b0, b1))\n",
    "    loss_history.append(loss)\n",
    "    # gradient descent\n",
    "    b0 += 2 * lr * np.sum(res) / y.shape[0]\n",
    "    b1 += 2 * lr * np.sum(res * x) / y.shape[0]\n",
    "print('Loss = %.4f' %loss)\n",
    "print(\"b0=%.4f\" %b0)\n",
    "print(\"b1=%.4f\" %b1)\n",
    "for i in range(400):\n",
    "    if (i+1) % 10 == 0:\n",
    "        print(\"Iteration %3d: Loss = %.4f\" %(i+1, loss_history[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a contour plot to demonstrate gradient descent\n",
    "levels = [0, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 40.0, 60.0, 80.0, 100.0]\n",
    "w0 = np.linspace(-10,10, 101)\n",
    "w1 = np.linspace(-10,10, 101)\n",
    "B0, B1 = np.meshgrid(w0, w1)\n",
    "f = np.sum(np.square(y - (B0[:, :, np.newaxis] + B1[:, :, np.newaxis] * x)), axis = 2) / y.shape\n",
    "cp = plt.contour(B0, B1, f, levels)\n",
    "plt.clabel(cp, inline=1, fontsize=8)\n",
    "plt.axhline(0, color='black', alpha=.5, dashes=[2, 4],linewidth=1)\n",
    "plt.axvline(0, color='black', alpha=0.5, dashes=[2, 4],linewidth=1)\n",
    "for i in range(5):\n",
    "    plt.annotate('', xy=para_history[i+1], xytext = para_history[i],\n",
    "                 arrowprops={'arrowstyle': '->', 'color': 'r', 'lw': 1},\n",
    "                 va='center', ha='center')\n",
    "plt.xlabel('w0')\n",
    "plt.ylabel('w1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
