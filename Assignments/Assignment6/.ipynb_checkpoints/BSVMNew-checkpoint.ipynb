{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Support Vector Machine from Scratch in Python\n",
    "\n",
    "Support Vector Machine (SVM) is a popular classification algorithm. Here we implement the algorithm for binary classification problems (i.e., there are two classes only). A toy dataset of six observations with two features is used to test the algorithm.\n",
    "\n",
    "## Notation\n",
    "-  Features: $X\\in $$R^{m \\times n}$ is a matrix for $m$ observations with $n$ features.\n",
    "-  Labels: $y\\in $$R^{m}$ is a vector of class labels (1 for the positive class, and -1 for the negative class). \n",
    "-  Parameters: $w\\in $$R^{n}$ is a vector of the coefficients, while $b\\in R$ is the bias term.\n",
    "\n",
    "## Algorithm\n",
    "-  Score function<br/>\n",
    "The separating hyperplane is formulated as $w^{T}x + b = 0$ for $x\\in R^{n}$. Let column vector $x_{i}$ contain the features of the $i$th observation. Its score is evaluated as $s_{i} = w^{T}x_{i} + b$. \n",
    "-  Loss function<br/>\n",
    "An observation $i$ is predicted to be positive if its score $s_{i} \\geq 0$ and negative if its score $s_{i} < 0$. Therefore, observation $i$ is correctly classified if $y_{i}s_{i} \\geq 0$. In SVM, we want that the distance from each observation to the separating hyperplane should be greater than or equal to the margin. Denote the margin by $\\Delta$. The SVM loss function is formulated as $L=\\frac{1}{m}\\sum_{i=1}^{m}\\max(0, \\Delta-y_{i}s_{i})$$=\\frac{1}{m}\\sum_{i=1}^{m}\\max(0, \\Delta-y_{i}(w^{T}x_{i} + b))$. Note that the loss of an observation $i$ is zero if $y_{i}s_{i} \\geq \\Delta$. Such a loss function is called hinge loss. \n",
    "-  Optimization<br/>\n",
    "We can use the loss function to quantify the quality of parameters $w$ and $b$. The objective of optimization is to find $w$ and $b$ that minimize the loss. Gradient descent is applied to minimize the loss function in our implementation. We differentiate the loss function with respect to the parameters and obtain: $\\nabla_{w}L$$=-\\frac{1}{m}\\sum_{i=1}^{m}1(\\Delta-y_{i}s_{i}>0)y_{i}x_{i}$, $\\nabla_{b}L$$=-\\frac{1}{m}\\sum_{i=1}^{m}1(\\Delta-y_{i}s_{i}>0)y_{i}$, where the indicator function $1(\\Delta-y_{i}s_{i}>0)$ returns 1 if $y_{i}s_{i}<\\Delta$, and returns 0 otherwise. In the implementation below, the margin $\\Delta$ is chosen to be 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1. In the next code cell, a dataset is constructed.  \n",
    "1. What is the shape of the Numpy ndarray `X`? \n",
    "2. How many observations and features are there in the dataset?\n",
    "3. What is the shape of the Numpy ndarray `y`?\n",
    "4. How many observations are in the positive class? How many observations are in the negative class?\n",
    "\n",
    "**Your Answers:** *fill in here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smsize = 6\n",
    "features = 2\n",
    "X = np.array([[1,7], [2,8], [3,8],\n",
    "             [5,1], [6,-1], [7,3]])\n",
    "# positive class: y = 1; negative class: y = -1\n",
    "y = np.array([-1, -1, -1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Separating Hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2. In the next code cell, a scatter plot is created.  \n",
    "1. What color represents positive class observations in the scatter plot below? \n",
    "2. What color represents negative class observations in the scatter plot below?\n",
    "3. In the figure, the decision boundary is formulated as $0.2295x_1-0.2129x_2+0.03=0$. Use this decision boundary to compute the score of the first observation (1, 7) in the dataset. (Hint: refer to the score function given in the first cell.) Per the score, what is the predicted class of this observation? Is the observation misclassified?\n",
    "4. Clearly, the dataset (`X`, `y`) constructed above is linearly separable. Can you revise one and only one element in the Numpy ndarray `y` such that the dataset is no longer linearly separable? Explain.   \n",
    "\n",
    "**Your Answers:** *fill in here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a scatter plot\n",
    "fig = plt.figure(figsize = (10, 5))   \n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "for i in range(smsize):\n",
    "    if y[i] == 1:\n",
    "        ax.plot(X[i,0], X[i,1], 'o', color='r')\n",
    "    else:\n",
    "        ax.plot(X[i,0], X[i,1], 'X', color='g')\n",
    "# Linear SVM\n",
    "clf = LinearSVC()\n",
    "clf.fit(X, y)\n",
    "wv = np.array(clf.coef_).reshape(2,)\n",
    "bv = clf.intercept_\n",
    "ax.text(3, 3, '%+.4fx1%+.4fx2%+4.2f=0' %(wv[0], wv[1], bv))\n",
    "x2 = np.linspace(0, 7, 15)\n",
    "y2 = np.dot(x2, -wv[0]/wv[1]) - bv/wv[1]\n",
    "ax.plot(x2, y2, 'b-')\n",
    "# configuring the plot\n",
    "ax.set_xlabel('X1')\n",
    "ax.set_ylabel('X2')\n",
    "ax.set_title('Support Vector Machine')\n",
    "ax.set_ylim(-2, 10)\n",
    "ax.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Support Vector Machine - Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(w, b, X):\n",
    "    #####################################\n",
    "    # TODO 1:                           #\n",
    "    # Compute and return a 1-d array    #\n",
    "    # of the scores. Use only basic     #\n",
    "    # Numpy array operations. No loop   #\n",
    "    # is allowed.                       #\n",
    "    #####################################\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(w, b, s, y, delta):\n",
    "    z = delta - y * s\n",
    "    #####################################\n",
    "    # TODO 2:                           #\n",
    "    # Compute and return the loss. Use  #\n",
    "    # only basic Numpy array operations.# \n",
    "    # No loop is allowed. Hint: a       #\n",
    "    # boolean mask can be applied to    #\n",
    "    # implement the max operator.       #\n",
    "    #####################################\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(w, b, X, s, y, delta):\n",
    "    z = delta - y * s\n",
    "    #####################################\n",
    "    # TODO 3:                           #\n",
    "    # Compute the gradient dw and db.   #\n",
    "    # Use only basic Numpy array        #\n",
    "    # operations. No loop is allowed.   #\n",
    "    # Hint: a boolean mask can be       #\n",
    "    # applied to implement the          #\n",
    "    # indicator function.               #\n",
    "    #####################################\n",
    "    pass\n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3. After reading the SVM algorithm description at the beginning of this notebook, complete TODO 1, TODO 2 and TODO 3 in functions *score*, *loss* and *gradient*. Then run the code below (hint: click the Cell tab, and select Run All). The loss should be decreasing and reach 0 after several iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_history = []\n",
    "loss_history = []\n",
    "# learning rate\n",
    "lr = 1.0e-2\n",
    "# margin\n",
    "delta = 1.0\n",
    "# initialization\n",
    "b = 0.0\n",
    "w = np.ones(X.shape[1]) * 1.0e-3\n",
    "for t in range(20):\n",
    "    # s is a 1-d array of scores\n",
    "    s = score(w, b, X)\n",
    "    # L is the loss\n",
    "    L = loss(w, b, s, y, delta)\n",
    "    # para_history and loss_history store historical results\n",
    "    para_history.append((w, b))\n",
    "    loss_history.append(L)\n",
    "    # compute gradient\n",
    "    dw, db = gradient(w, b, X, s, y, delta)\n",
    "    # gradient descent\n",
    "    w -= lr * dw\n",
    "    b -= lr * db\n",
    "print('Loss = %.4f' %L)\n",
    "print('Parameters:')\n",
    "print(\"b=%.4f\" %b)\n",
    "for j in range(w.shape[0]):\n",
    "    print(\"w[%2d]=%.4f\" %(j, w[j]))\n",
    "print(\"Loss history:\")\n",
    "for i in range(20):\n",
    "    if (i+1) % 1 == 0:\n",
    "        print(\"Iteration %3d: Loss = %.4f\" %(i+1, loss_history[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
